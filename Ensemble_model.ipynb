{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 21:20:44.066401: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 21:20:44.077390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740325844.091708   21226 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740325844.095588   21226 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 21:20:44.109789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ensemble classifier...\n",
      "Loading Wav2Vec2 model...\n",
      "Loading YAMNet model...\n",
      "\n",
      "Creating test data loader...\n",
      "Loaded dataset with 671 samples\n",
      "Label mapping: {'crying': 0, 'normal': 1, 'screaming': 2}\n",
      "\n",
      "Getting model predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84696080ed4a45e3a1e17f19706ebd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting wav2vec predictions:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed248daafec4add80c76d80fde089b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting yamnet predictions:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200><function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200><function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200><function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200>\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "      File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "self._shutdown_workers()    \n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "self._shutdown_workers()  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "\n",
      "              File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    self._shutdown_workers()if w.is_alive():self._shutdown_workers()\n",
      "\n",
      "\n",
      "if w.is_alive():  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "\n",
      "      File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():if w.is_alive():    \n",
      "\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "AssertionError  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError:         can only test a child process: assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError\n",
      "can only test a child process: AssertionError\n",
      ": can only test a child processcan only test a child process\n",
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200>\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "Exception ignored in:   File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200>Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200><function _MultiProcessingDataLoaderIter.__del__ at 0x7cacfd71a200>\n",
      "\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "self._shutdown_workers()Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "      File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "\n",
      "self._shutdown_workers()  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "        \n",
      "self._shutdown_workers()  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "if w.is_alive():      File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "\n",
      "\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "if w.is_alive():  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "        if w.is_alive():\n",
      "if w.is_alive():    \n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "      File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process':     \n",
      "can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError\n",
      ": can only test a child process\n",
      "\n",
      "AssertionErrorAssertionError: can only test a child process: \n",
      "can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating validation data loader for meta-classifier training...\n",
      "Loaded dataset with 670 samples\n",
      "Label mapping: {'crying': 0, 'normal': 1, 'screaming': 2}\n",
      "\n",
      "Getting validation set predictions for meta-classifier training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d1e8bde21a41f49927ac753cca84c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting wav2vec predictions:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e446e26181417a8267186e74800ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting yamnet predictions:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training meta-classifier...\n",
      "\n",
      "Evaluating ensemble methods...\n",
      "\n",
      "Generating visualizations...\n",
      "\n",
      "Best performing method: meta_classifier\n",
      "F1 Score: 0.9568\n",
      "Parameters:\n",
      "  type: RandomForestClassifier\n",
      "  params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "Results saved to ensemble_output/evaluation_results.json\n",
      "\n",
      "Ensemble evaluation complete! Check the output directory for visualizations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import atexit\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import contextlib\n",
    "\n",
    "# Initialize colorama\n",
    "colorama.init()\n",
    "\n",
    "def cleanup_dataloader():\n",
    "    torch.utils.data._utils.worker._worker_info = None\n",
    "\n",
    "# Register cleanup function\n",
    "atexit.register(cleanup_dataloader)\n",
    "\n",
    "class SuppressOutput:\n",
    "    \"\"\"Context manager to suppress stdout and stderr\"\"\"\n",
    "    def __init__(self):\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "\n",
    "    def __enter__(self):\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        for fd in self.null_fds + self.save_fds:\n",
    "            os.close(fd)\n",
    "        warnings.resetwarnings()\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"Handles loading of pretrained models\"\"\"\n",
    "    @staticmethod\n",
    "    def load_wav2vec2(model_path: str) -> Tuple[nn.Module, Any]:\n",
    "        \"\"\"Load Wav2Vec2 model and feature extractor\"\"\"\n",
    "        try:\n",
    "            # Load model\n",
    "            model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n",
    "            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
    "            \n",
    "            # Load label map\n",
    "            with open(os.path.join(model_path, \"label_map.json\"), \"r\") as f:\n",
    "                label_map = json.load(f)\n",
    "            \n",
    "            return model, feature_extractor, label_map\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading Wav2Vec2 model: {str(e)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_yamnet(model_path: str) -> nn.Module:\n",
    "        \"\"\"Load YAMNet model\"\"\"\n",
    "        try:\n",
    "            from yamnet import YAMNetBase  # Import from previous script\n",
    "            \n",
    "            # Load label map\n",
    "            with open(os.path.join(model_path, \"label_map.json\"), \"r\") as f:\n",
    "                label_map = json.load(f)\n",
    "            \n",
    "            # Initialize model\n",
    "            model = YAMNetBase(num_classes=len(label_map))\n",
    "            \n",
    "            # Load state dict\n",
    "            model.load_state_dict(torch.load(os.path.join(model_path, \"model.pt\")))\n",
    "            \n",
    "            return model, label_map\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading YAMNet model: {str(e)}\")\n",
    "\n",
    "class Visualizer:\n",
    "    \"\"\"Enhanced visualization capabilities\"\"\"\n",
    "    def __init__(self, label_map: Dict[str, int]):\n",
    "        self.label_map = label_map\n",
    "        self.rev_label_map = {v: k for k, v in label_map.items()}\n",
    "        \n",
    "        # Set default matplotlib style\n",
    "        plt.style.use('default')\n",
    "        # Apply seaborn styling\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    def plot_confusion_matrix(\n",
    "        self,\n",
    "        cm: np.ndarray,\n",
    "        method_name: str,\n",
    "        output_dir: str\n",
    "    ) -> None:\n",
    "        \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=self.label_map.keys(),\n",
    "            yticklabels=self.label_map.keys()\n",
    "        )\n",
    "        plt.title(f'Confusion Matrix - {method_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()  # Added to prevent label cutoff\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(output_dir, f'confusion_matrix_{method_name}.png'), \n",
    "                    bbox_inches='tight')  # Added to ensure full save\n",
    "        plt.close()\n",
    "\n",
    "    def plot_roc_curves(\n",
    "        self,\n",
    "        probabilities: Dict[str, np.ndarray],\n",
    "        true_labels: np.ndarray,\n",
    "        output_dir: str\n",
    "    ) -> None:\n",
    "        \"\"\"Plot ROC curves for all methods\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Use a color palette for consistent colors\n",
    "        colors = sns.color_palette(\"husl\", n_colors=len(probabilities)*len(self.label_map))\n",
    "        color_idx = 0\n",
    "        \n",
    "        for method_name, probs in probabilities.items():\n",
    "            for i in range(len(self.label_map)):\n",
    "                # Prepare binary labels\n",
    "                binary_labels = (true_labels == i).astype(int)\n",
    "                \n",
    "                # Calculate ROC curve\n",
    "                if method_name in ['majority_voting', 'meta_classifier']:\n",
    "                    # For discrete predictions, create pseudo-probabilities\n",
    "                    class_probs = (probs == i).astype(float)\n",
    "                else:\n",
    "                    class_probs = probs[:, i]\n",
    "                \n",
    "                try:\n",
    "                    fpr, tpr, _ = roc_curve(binary_labels, class_probs)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve with consistent color\n",
    "                    plt.plot(\n",
    "                        fpr,\n",
    "                        tpr,\n",
    "                        color=colors[color_idx],\n",
    "                        label=f'{method_name} - {self.rev_label_map[i]} (AUC = {roc_auc:.2f})'\n",
    "                    )\n",
    "                    color_idx += 1\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not compute ROC curve for {method_name} - {self.rev_label_map[i]}: {str(e)}\")\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves for All Methods')\n",
    "        plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(output_dir, 'roc_curves.png'),\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=300)  # Increased DPI for better quality\n",
    "        plt.close()\n",
    "\n",
    "    def plot_performance_comparison(\n",
    "        self,\n",
    "        results: Dict[str, Dict[str, float]],\n",
    "        output_dir: str\n",
    "    ) -> None:\n",
    "        \"\"\"Plot performance metrics comparison\"\"\"\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "        # Filter out the 'best_method' entry since it has a different structure\n",
    "        methods = [method for method in results.keys() if method != 'best_method']\n",
    "    \n",
    "        # Prepare data for plotting\n",
    "        data = []\n",
    "        for method in methods:\n",
    "            for metric in metrics:\n",
    "                data.append({\n",
    "                    'Method': method,\n",
    "                    'Metric': metric,\n",
    "                    'Score': results[method][metric]\n",
    "                })\n",
    "    \n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "        # Create grouped bar plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        g = sns.barplot(\n",
    "            x='Method',\n",
    "            y='Score',\n",
    "            hue='Metric',\n",
    "            data=df,\n",
    "            palette='husl'\n",
    "        )\n",
    "    \n",
    "        plt.title('Performance Comparison Across Methods')\n",
    "        plt.xticks(rotation=45, ha='right')  # Improved readability\n",
    "    \n",
    "        # Add value labels on the bars\n",
    "        for container in g.containers:\n",
    "            g.bar_label(container, fmt='%.2f', padding=3)\n",
    "    \n",
    "        plt.tight_layout()\n",
    "    \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(output_dir, 'performance_comparison.png'),\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_prediction_distribution(\n",
    "        self,\n",
    "        predictions: Dict[str, np.ndarray],\n",
    "        output_dir: str\n",
    "    ) -> None:\n",
    "        \"\"\"Plot distribution of predictions for each method\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        x = np.arange(len(self.label_map))\n",
    "        width = 0.8 / len(predictions)\n",
    "        \n",
    "        # Use a consistent color palette\n",
    "        colors = sns.color_palette(\"husl\", n_colors=len(predictions))\n",
    "        \n",
    "        for i, (method, preds) in enumerate(predictions.items()):\n",
    "            counts = np.bincount(preds, minlength=len(self.label_map))\n",
    "            plt.bar(\n",
    "                x + i * width,\n",
    "                counts,\n",
    "                width,\n",
    "                label=method,\n",
    "                color=colors[i],\n",
    "                alpha=0.8\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Classes')\n",
    "        plt.ylabel('Number of Predictions')\n",
    "        plt.title('Distribution of Predictions Across Methods')\n",
    "        plt.xticks(x + width * len(predictions) / 2, \n",
    "                   self.label_map.keys(),\n",
    "                   rotation=45,\n",
    "                   ha='right')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(output_dir, 'prediction_distribution.png'),\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "class EnsembleClassifier:\n",
    "    \"\"\"Enhanced Ensemble Classifier with prediction capabilities\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav2vec_path: str,\n",
    "        yamnet_path: str,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load models\n",
    "        print(\"Loading Wav2Vec2 model...\")\n",
    "        self.wav2vec_model, self.feature_extractor, self.label_map = ModelLoader.load_wav2vec2(wav2vec_path)\n",
    "        self.wav2vec_model.to(self.device)\n",
    "        self.wav2vec_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(\"Loading YAMNet model...\")\n",
    "        self.yamnet_model, yamnet_label_map = ModelLoader.load_yamnet(yamnet_path)\n",
    "        self.yamnet_model.to(self.device)\n",
    "        self.yamnet_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Initialize other components\n",
    "        self.meta_classifier = None\n",
    "        self.visualizer = Visualizer(self.label_map)\n",
    "\n",
    "    def get_model_predictions(\n",
    "        self,\n",
    "        data_loader: torch.utils.data.DataLoader,\n",
    "        model_type: str\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get predictions from specified model\n",
    "        \n",
    "        Args:\n",
    "            data_loader: DataLoader containing test data\n",
    "            model_type: Either 'wav2vec' or 'yamnet'\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predictions probabilities, true labels)\n",
    "        \"\"\"\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=f\"Getting {model_type} predictions\"):\n",
    "                inputs, labels = batch\n",
    "                \n",
    "                if model_type == 'wav2vec':\n",
    "                    # Convert batch tensor to list of numpy arrays\n",
    "                    input_list = [x.numpy() for x in inputs]\n",
    "                    \n",
    "                    # Process audio for Wav2Vec2\n",
    "                    features = self.feature_extractor(\n",
    "                        input_list,\n",
    "                        sampling_rate=16000,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True\n",
    "                    )\n",
    "                    features = {k: v.to(self.device) for k, v in features.items()}\n",
    "                    \n",
    "                    outputs = self.wav2vec_model(**features)\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                elif model_type == 'yamnet':\n",
    "                    # Ensure input tensor has the correct shape [batch_size, 1, time_steps]\n",
    "                    if inputs.dim() == 2:  # If input is [batch_size, time_steps]\n",
    "                        inputs = inputs.unsqueeze(1)  # Add channel dimension\n",
    "                    elif inputs.dim() == 3 and inputs.size(1) != 1:  # If input is [batch_size, channels, time_steps]\n",
    "                        # Average across channels to convert to mono\n",
    "                        inputs = inputs.mean(dim=1, keepdim=True)\n",
    "                    \n",
    "                    # Move inputs to the correct device\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    \n",
    "                    # Get predictions from YAMNet\n",
    "                    logits = self.yamnet_model(inputs)\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "                \n",
    "                # Convert logits to probabilities\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(all_probs), np.concatenate(all_labels)\n",
    "\n",
    "    def majority_voting(\n",
    "        self,\n",
    "        wav2vec_preds: np.ndarray,\n",
    "        yamnet_preds: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Combine predictions using majority voting\"\"\"\n",
    "        # Stack predictions and take mode along axis 1\n",
    "        stacked_preds = np.vstack([wav2vec_preds, yamnet_preds])\n",
    "        return np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(),\n",
    "            axis=0,\n",
    "            arr=stacked_preds\n",
    "        )\n",
    "\n",
    "    def weighted_average(\n",
    "        self,\n",
    "        wav2vec_probs: np.ndarray,\n",
    "        yamnet_probs: np.ndarray,\n",
    "        wav2vec_weight: float = 0.6\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Combine predictions using weighted average of probabilities\"\"\"\n",
    "        yamnet_weight = 1 - wav2vec_weight\n",
    "        weighted_probs = (wav2vec_weight * wav2vec_probs + \n",
    "                         yamnet_weight * yamnet_probs)\n",
    "        return np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "    def train_meta_classifier(\n",
    "        self,\n",
    "        wav2vec_probs: np.ndarray,\n",
    "        yamnet_probs: np.ndarray,\n",
    "        true_labels: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Train meta-classifier on combined probabilities\"\"\"\n",
    "        # Combine features from both models\n",
    "        combined_features = np.hstack([wav2vec_probs, yamnet_probs])\n",
    "        \n",
    "        # Initialize and train meta-classifier\n",
    "        self.meta_classifier = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.meta_classifier.fit(combined_features, true_labels)\n",
    "\n",
    "    def evaluate_ensemble(\n",
    "        self,\n",
    "        wav2vec_probs: np.ndarray,\n",
    "        yamnet_probs: np.ndarray,\n",
    "        true_labels: np.ndarray\n",
    "    ) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Evaluate all ensemble methods\"\"\"\n",
    "        results = {}\n",
    "        best_method = None\n",
    "        best_f1 = -1\n",
    "        ensemble_params = {}\n",
    "    \n",
    "        # Get predictions for each method\n",
    "        predictions = {\n",
    "            'wav2vec': np.argmax(wav2vec_probs, axis=1),\n",
    "            'yamnet': np.argmax(yamnet_probs, axis=1),\n",
    "            'majority_voting': self.majority_voting(\n",
    "                np.argmax(wav2vec_probs, axis=1),\n",
    "                np.argmax(yamnet_probs, axis=1)\n",
    "            ),\n",
    "            'weighted_average': self.weighted_average(wav2vec_probs, yamnet_probs, wav2vec_weight=0.6)\n",
    "        }\n",
    "    \n",
    "        ensemble_params['weighted_average'] = {'wav2vec_weight': 0.6}\n",
    "    \n",
    "        if self.meta_classifier is not None:\n",
    "            combined_features = np.hstack([wav2vec_probs, yamnet_probs])\n",
    "            predictions['meta_classifier'] = self.meta_classifier.predict(combined_features)\n",
    "            ensemble_params['meta_classifier'] = {\n",
    "                'type': type(self.meta_classifier).__name__,\n",
    "                'params': self.meta_classifier.get_params()\n",
    "            }\n",
    "    \n",
    "        # Calculate metrics for each method\n",
    "        for method, preds in predictions.items():\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                true_labels,\n",
    "                preds,\n",
    "                average='weighted'\n",
    "            )\n",
    "        \n",
    "            results[method] = {\n",
    "                'accuracy': accuracy_score(true_labels, preds),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "        \n",
    "            # Track best method\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_method = method\n",
    "    \n",
    "        # Add best method info to results\n",
    "        results['best_method'] = {\n",
    "            'name': best_method,\n",
    "            'f1_score': best_f1,\n",
    "            'parameters': ensemble_params.get(best_method, {})\n",
    "        }\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict[str, Dict[str, float]], output_dir: str) -> None:\n",
    "        \"\"\"Save evaluation results to JSON file\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, 'evaluation_results.json')\n",
    "    \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "    \n",
    "        # Print best method information\n",
    "        best_method = results['best_method']\n",
    "        print(f\"\\nBest performing method: {best_method['name']}\")\n",
    "        print(f\"F1 Score: {best_method['f1_score']:.4f}\")\n",
    "        if best_method['parameters']:\n",
    "            print(\"Parameters:\")\n",
    "            for param, value in best_method['parameters'].items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "    \n",
    "        print(f\"\\nResults saved to {output_path}\")\n",
    "\n",
    "    def process_audio(\n",
    "        self,\n",
    "        audio_path: str,\n",
    "        target_sr: int = 16000\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Process audio file for model input\"\"\"\n",
    "        # Load and resample audio\n",
    "        audio, sr = librosa.load(audio_path, sr=None)\n",
    "        if sr != target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        audio_tensor = torch.FloatTensor(audio)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "class AudioDatasetWithMetadata(Dataset):\n",
    "    \"\"\"Dataset for loading preprocessed audio files with metadata\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dir: str,\n",
    "        metadata_file: str,\n",
    "        sample_rate: int = 16000\n",
    "    ):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_file)\n",
    "        \n",
    "        # Create label mapping\n",
    "        unique_labels = sorted(self.metadata['label'].unique())\n",
    "        self.label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        \n",
    "        print(f\"Loaded dataset with {len(self.metadata)} samples\")\n",
    "        print(\"Label mapping:\", self.label_map)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        row = self.metadata.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, row['file_name'])\n",
    "        label = self.label_map[row['label']]\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            audio_tensor = torch.FloatTensor(audio)\n",
    "            \n",
    "            return audio_tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {str(e)}\")\n",
    "            # Return a zero tensor and the label if loading fails\n",
    "            return torch.zeros(self.sample_rate * 2), label  # 2 seconds of zeros\n",
    "        \n",
    "def cleanup_dataloader():\n",
    "    torch.utils.data._utils.worker._worker_info = None\n",
    "\n",
    "def main():\n",
    "    # Set paths\n",
    "    base_dir = \"Split_Data\"\n",
    "    wav2vec_path = \"model_output/best_model\"\n",
    "    yamnet_path = \"yamnet_model_output/best_model\"\n",
    "    output_dir = \"ensemble_output\"\n",
    "\n",
    "    # Register cleanup function\n",
    "    atexit.register(cleanup_dataloader)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Initialize ensemble classifier\n",
    "        print(\"Initializing ensemble classifier...\")\n",
    "        ensemble = EnsembleClassifier(wav2vec_path, yamnet_path)\n",
    "        \n",
    "        # Create test dataset and dataloader\n",
    "        print(\"\\nCreating test data loader...\")\n",
    "        test_dataset = AudioDatasetWithMetadata(\n",
    "            audio_dir=os.path.join(base_dir, \"test\"),\n",
    "            metadata_file=os.path.join(base_dir, \"test_metadata.csv\"),\n",
    "            sample_rate=16000\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Get predictions from both models\n",
    "        print(\"\\nGetting model predictions...\")\n",
    "        wav2vec_probs, true_labels = ensemble.get_model_predictions(\n",
    "            test_loader, 'wav2vec'\n",
    "        )\n",
    "        yamnet_probs, _ = ensemble.get_model_predictions(\n",
    "            test_loader, 'yamnet'\n",
    "        )\n",
    "        \n",
    "        # Train meta-classifier using validation set\n",
    "        print(\"\\nCreating validation data loader for meta-classifier training...\")\n",
    "        val_dataset = AudioDatasetWithMetadata(\n",
    "            audio_dir=os.path.join(base_dir, \"val\"),\n",
    "            metadata_file=os.path.join(base_dir, \"val_metadata.csv\"),\n",
    "            sample_rate=16000\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0, # Disable workers for smaller dataset\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nGetting validation set predictions for meta-classifier training...\")\n",
    "        val_wav2vec_probs, val_labels = ensemble.get_model_predictions(\n",
    "            val_loader, 'wav2vec'\n",
    "        )\n",
    "        val_yamnet_probs, _ = ensemble.get_model_predictions(\n",
    "            val_loader, 'yamnet'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTraining meta-classifier...\")\n",
    "        ensemble.train_meta_classifier(\n",
    "            val_wav2vec_probs,\n",
    "            val_yamnet_probs,\n",
    "            val_labels\n",
    "        )\n",
    "        \n",
    "        # Evaluate ensemble methods\n",
    "        print(\"\\nEvaluating ensemble methods...\")\n",
    "        results = ensemble.evaluate_ensemble(\n",
    "            wav2vec_probs,\n",
    "            yamnet_probs,\n",
    "            true_labels\n",
    "        )\n",
    "        \n",
    "        # Generate visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        ensemble.visualizer.plot_confusion_matrix(\n",
    "            confusion_matrix(true_labels, np.argmax(wav2vec_probs, axis=1)),\n",
    "            method_name=\"Wav2Vec2\",\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        ensemble.visualizer.plot_confusion_matrix(\n",
    "            confusion_matrix(true_labels, np.argmax(yamnet_probs, axis=1)),\n",
    "            method_name=\"YAMNet\",\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        ensemble.visualizer.plot_roc_curves(\n",
    "            {\n",
    "                \"Wav2Vec2\": wav2vec_probs,\n",
    "                \"YAMNet\": yamnet_probs\n",
    "            },\n",
    "            true_labels,\n",
    "            output_dir\n",
    "        )\n",
    "        ensemble.visualizer.plot_performance_comparison(\n",
    "            results,\n",
    "            output_dir\n",
    "        )\n",
    "        ensemble.visualizer.plot_prediction_distribution(\n",
    "            {\n",
    "                \"Wav2Vec2\": np.argmax(wav2vec_probs, axis=1),\n",
    "                \"YAMNet\": np.argmax(yamnet_probs, axis=1)\n",
    "            },\n",
    "            output_dir\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        ensemble.save_results(results, output_dir)\n",
    "        \n",
    "        # Save label mapping\n",
    "        with open(os.path.join(output_dir, 'label_map.json'), 'w') as f:\n",
    "            json.dump(test_dataset.label_map, f, indent=4)\n",
    "        \n",
    "        print(\"\\nEnsemble evaluation complete! Check the output directory for visualizations.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        with SuppressOutput():\n",
    "            cleanup_dataloader()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
