{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: problems found:\n",
      "        - require? \u001b[31m X\u001b[0m jupyter-js-widgets/extension\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 18:14:09.161382: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 18:14:09.285832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740314649.334462    2925 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740314649.347980    2925 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 18:14:09.452147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing datasets with augmentation...\n",
      "Loading metadata from Split_Data/train_metadata.csv\n",
      "Loaded 3129 entries from metadata\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2110e14e244efd90dcbae07e88d9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading audio files:   0%|          | 0/3129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2925/3630925577.py:297: RuntimeWarning: invalid value encountered in divide\n",
      "  audio = audio / np.max(np.abs(audio))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 6258 examples\n",
      "\n",
      "Starting k-fold cross validation training...\n",
      "\n",
      "Training Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/3900 11:16 < 37:40, 1.33 it/s, Epoch 5/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.000600</td>\n",
       "      <td>0.902244</td>\n",
       "      <td>0.719649</td>\n",
       "      <td>0.711932</td>\n",
       "      <td>0.743188</td>\n",
       "      <td>0.719649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.622062</td>\n",
       "      <td>0.797125</td>\n",
       "      <td>0.788686</td>\n",
       "      <td>0.817309</td>\n",
       "      <td>0.797125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.344257</td>\n",
       "      <td>0.904952</td>\n",
       "      <td>0.904387</td>\n",
       "      <td>0.904859</td>\n",
       "      <td>0.904952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.918530</td>\n",
       "      <td>0.918253</td>\n",
       "      <td>0.921105</td>\n",
       "      <td>0.918530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.426728</td>\n",
       "      <td>0.884984</td>\n",
       "      <td>0.883898</td>\n",
       "      <td>0.901551</td>\n",
       "      <td>0.884984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.207235</td>\n",
       "      <td>0.942492</td>\n",
       "      <td>0.942307</td>\n",
       "      <td>0.942913</td>\n",
       "      <td>0.942492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.207168</td>\n",
       "      <td>0.951278</td>\n",
       "      <td>0.951271</td>\n",
       "      <td>0.951916</td>\n",
       "      <td>0.951278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.396919</td>\n",
       "      <td>0.906550</td>\n",
       "      <td>0.906026</td>\n",
       "      <td>0.912777</td>\n",
       "      <td>0.906550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.219396</td>\n",
       "      <td>0.944888</td>\n",
       "      <td>0.944888</td>\n",
       "      <td>0.947066</td>\n",
       "      <td>0.944888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training History:\n",
      "----------------------------------------\n",
      "Epoch  1: Loss: 1.1029 Val Loss: 0.9022439122200012 Acc: 0.7196485623003195\n",
      "Epoch  2: Loss: 1.0006 Val Loss: 0.6220619082450867 Acc: 0.7971246006389776\n",
      "Epoch  3: Loss: 0.8260 Val Loss: 0.3442566990852356 Acc: 0.9049520766773163\n",
      "Epoch  4: Loss: 0.6415 Val Loss: 0.28159987926483154 Acc: 0.9185303514376997\n",
      "Epoch  5: Loss: 0.4876 Val Loss: 0.4267275929450989 Acc: 0.8849840255591054\n",
      "Epoch  6: Loss: 0.4431 Val Loss: 0.2072354406118393 Acc: 0.9424920127795527\n",
      "Epoch  7: Loss: 0.3304 Val Loss: 0.2071676105260849 Acc: 0.9512779552715654\n",
      "Epoch  8: Loss: 0.3311 Val Loss: 0.396919310092926 Acc: 0.9065495207667732\n",
      "Epoch  9: Loss: 0.2534 Val Loss: 0.21939603984355927 Acc: 0.944888178913738\n",
      "Epoch 10: Loss: 0.2672 Val Loss: 0.2071676105260849 Acc: 0.9512779552715654\n",
      "Epoch 11: Loss: 0.3211 Val Loss: N/A Acc: N/A\n",
      "Epoch 12: Loss: 0.2560 Val Loss: N/A Acc: N/A\n",
      "Epoch 13: Loss: 0.1799 Val Loss: N/A Acc: N/A\n",
      "Epoch 14: Loss: 0.2066 Val Loss: N/A Acc: N/A\n",
      "Epoch 15: Loss: 0.1822 Val Loss: N/A Acc: N/A\n",
      "Epoch 16: Loss: 0.1621 Val Loss: N/A Acc: N/A\n",
      "Epoch 17: Loss: 0.1810 Val Loss: N/A Acc: N/A\n",
      "Epoch 18: Loss: 0.1588 Val Loss: N/A Acc: N/A\n",
      "\n",
      "Training Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/3900 05:02 < 44:21, 1.32 it/s, Epoch 2/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.134247</td>\n",
       "      <td>0.968051</td>\n",
       "      <td>0.967966</td>\n",
       "      <td>0.968050</td>\n",
       "      <td>0.968051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>0.120265</td>\n",
       "      <td>0.968051</td>\n",
       "      <td>0.968066</td>\n",
       "      <td>0.968137</td>\n",
       "      <td>0.968051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.148590</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.953487</td>\n",
       "      <td>0.956574</td>\n",
       "      <td>0.953674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.096173</td>\n",
       "      <td>0.977636</td>\n",
       "      <td>0.977616</td>\n",
       "      <td>0.977634</td>\n",
       "      <td>0.977636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training History:\n",
      "----------------------------------------\n",
      "Epoch  1: Loss: 0.1627 Val Loss: 0.13424740731716156 Acc: 0.9680511182108626\n",
      "Epoch  2: Loss: 0.1553 Val Loss: 0.12026499956846237 Acc: 0.9680511182108626\n",
      "Epoch  3: Loss: 0.1294 Val Loss: 0.14858995378017426 Acc: 0.9536741214057508\n",
      "Epoch  4: Loss: 0.1269 Val Loss: 0.09617292135953903 Acc: 0.9776357827476039\n",
      "Epoch  5: Loss: 0.1340 Val Loss: 0.09617292135953903 Acc: 0.9776357827476039\n",
      "Epoch  6: Loss: 0.1160 Val Loss: N/A Acc: N/A\n",
      "Epoch  7: Loss: 0.1269 Val Loss: N/A Acc: N/A\n",
      "Epoch  8: Loss: 0.1451 Val Loss: N/A Acc: N/A\n",
      "\n",
      "Training Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/3900 06:18 < 43:03, 1.32 it/s, Epoch 3/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>0.969799</td>\n",
       "      <td>0.970349</td>\n",
       "      <td>0.969649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.089811</td>\n",
       "      <td>0.980032</td>\n",
       "      <td>0.980035</td>\n",
       "      <td>0.980039</td>\n",
       "      <td>0.980032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.074448</td>\n",
       "      <td>0.984824</td>\n",
       "      <td>0.984824</td>\n",
       "      <td>0.984874</td>\n",
       "      <td>0.984824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.104307</td>\n",
       "      <td>0.980032</td>\n",
       "      <td>0.979936</td>\n",
       "      <td>0.980266</td>\n",
       "      <td>0.980032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.123600</td>\n",
       "      <td>0.189024</td>\n",
       "      <td>0.960064</td>\n",
       "      <td>0.959992</td>\n",
       "      <td>0.961431</td>\n",
       "      <td>0.960064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training History:\n",
      "----------------------------------------\n",
      "Epoch  1: Loss: 0.0944 Val Loss: 0.11264976859092712 Acc: 0.9696485623003195\n",
      "Epoch  2: Loss: 0.0844 Val Loss: 0.0898108184337616 Acc: 0.9800319488817891\n",
      "Epoch  3: Loss: 0.0785 Val Loss: 0.07444776594638824 Acc: 0.9848242811501597\n",
      "Epoch  4: Loss: 0.0669 Val Loss: 0.1043071448802948 Acc: 0.9800319488817891\n",
      "Epoch  5: Loss: 0.0692 Val Loss: 0.18902380764484406 Acc: 0.9600638977635783\n",
      "Epoch  6: Loss: 0.1204 Val Loss: 0.07444776594638824 Acc: 0.9848242811501597\n",
      "Epoch  7: Loss: 0.1054 Val Loss: N/A Acc: N/A\n",
      "Epoch  8: Loss: 0.0681 Val Loss: N/A Acc: N/A\n",
      "Epoch  9: Loss: 0.1769 Val Loss: N/A Acc: N/A\n",
      "Epoch 10: Loss: 0.1236 Val Loss: N/A Acc: N/A\n",
      "\n",
      "Training Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/3900 05:04 < 44:37, 1.31 it/s, Epoch 2/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.070544</td>\n",
       "      <td>0.987210</td>\n",
       "      <td>0.987240</td>\n",
       "      <td>0.987366</td>\n",
       "      <td>0.987210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.044151</td>\n",
       "      <td>0.988809</td>\n",
       "      <td>0.988815</td>\n",
       "      <td>0.988835</td>\n",
       "      <td>0.988809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.052872</td>\n",
       "      <td>0.983213</td>\n",
       "      <td>0.983220</td>\n",
       "      <td>0.983408</td>\n",
       "      <td>0.983213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.113385</td>\n",
       "      <td>0.972022</td>\n",
       "      <td>0.971954</td>\n",
       "      <td>0.972815</td>\n",
       "      <td>0.972022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training History:\n",
      "----------------------------------------\n",
      "Epoch  1: Loss: 0.0600 Val Loss: 0.0705437883734703 Acc: 0.9872102318145484\n",
      "Epoch  2: Loss: 0.0612 Val Loss: 0.044151317328214645 Acc: 0.9888089528377298\n",
      "Epoch  3: Loss: 0.0759 Val Loss: 0.0528721921145916 Acc: 0.9832134292565947\n",
      "Epoch  4: Loss: 0.0708 Val Loss: 0.11338482052087784 Acc: 0.9720223820943246\n",
      "Epoch  5: Loss: 0.0725 Val Loss: 0.044151317328214645 Acc: 0.9888089528377298\n",
      "Epoch  6: Loss: 0.1120 Val Loss: N/A Acc: N/A\n",
      "Epoch  7: Loss: 0.0839 Val Loss: N/A Acc: N/A\n",
      "Epoch  8: Loss: 0.0982 Val Loss: N/A Acc: N/A\n",
      "\n",
      "Training Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratyush/miniconda3/envs/tf_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='3900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/3900 05:45 < 50:40, 1.15 it/s, Epoch 2/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.987210</td>\n",
       "      <td>0.987234</td>\n",
       "      <td>0.987540</td>\n",
       "      <td>0.987210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.992812</td>\n",
       "      <td>0.992842</td>\n",
       "      <td>0.992806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.984013</td>\n",
       "      <td>0.984040</td>\n",
       "      <td>0.984400</td>\n",
       "      <td>0.984013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.984812</td>\n",
       "      <td>0.984812</td>\n",
       "      <td>0.984813</td>\n",
       "      <td>0.984812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training History:\n",
      "----------------------------------------\n",
      "Epoch  1: Loss: 0.0400 Val Loss: nan Acc: 0.9872102318145484\n",
      "Epoch  2: Loss: 0.0651 Val Loss: nan Acc: 0.9928057553956835\n",
      "Epoch  3: Loss: 0.0299 Val Loss: nan Acc: 0.9840127897681854\n",
      "Epoch  4: Loss: 0.0302 Val Loss: nan Acc: 0.9848121502797762\n",
      "Epoch  5: Loss: 0.0616 Val Loss: nan Acc: 0.9928057553956835\n",
      "Epoch  6: Loss: 0.0921 Val Loss: N/A Acc: N/A\n",
      "Epoch  7: Loss: 0.0922 Val Loss: N/A Acc: N/A\n",
      "Epoch  8: Loss: 0.1004 Val Loss: N/A Acc: N/A\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "{\n",
      "  \"eval_loss\": NaN,\n",
      "  \"eval_accuracy\": 0.9790705454805485,\n",
      "  \"eval_f1\": 0.9790674440522349,\n",
      "  \"eval_precision\": 0.9792200873731648,\n",
      "  \"eval_recall\": 0.9790705454805485,\n",
      "  \"eval_runtime\": 6.65872,\n",
      "  \"eval_samples_per_second\": 189.3056,\n",
      "  \"eval_steps_per_second\": 23.7458,\n",
      "  \"epoch\": 3.313738019169329\n",
      "}\n",
      "Model saved at model_output/best_model\n",
      "\n",
      "Best model saved at: model_output/best_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import Wav2Vec2FeatureExtractor, AutoConfig, EarlyStoppingCallback\n",
    "from transformers import Wav2Vec2ForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "# Initialize colorama for cross-platform colored output\n",
    "colorama.init()\n",
    "\n",
    "class AudioAugmenter:\n",
    "    \"\"\"Audio augmentation techniques with length preservation\"\"\"\n",
    "    @staticmethod\n",
    "    def pad_or_truncate(audio, max_length=32000):\n",
    "        if len(audio) > max_length:\n",
    "            return audio[:max_length]\n",
    "        elif len(audio) < max_length:\n",
    "            return np.pad(audio, (0, max_length - len(audio)), 'constant')\n",
    "        return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(audio, noise_factor=0.005):\n",
    "        try:\n",
    "            noise = np.random.randn(len(audio))\n",
    "            augmented = audio + noise_factor * noise\n",
    "            augmented = np.clip(augmented, -1.0, 1.0)\n",
    "            return augmented\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in add_noise: {str(e)}\")\n",
    "            return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_max=0.1):\n",
    "        try:\n",
    "            shift = int(len(audio) * shift_max)\n",
    "            return np.roll(audio, shift) if shift > 0 else audio\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in time_shift: {str(e)}\")\n",
    "            return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def change_speed(audio, speed_factor=0.2):\n",
    "        try:\n",
    "            if not np.all(np.isfinite(audio)):\n",
    "                audio = np.nan_to_num(audio, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "            audio = np.clip(audio, -1.0, 1.0)\n",
    "            speed_change = np.random.uniform(low=0.9, high=1.1)\n",
    "            \n",
    "            augmented = librosa.effects.time_stretch(audio, rate=speed_change)\n",
    "            augmented = np.nan_to_num(augmented, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            augmented = np.clip(augmented, -1.0, 1.0)\n",
    "            \n",
    "            # Ensure fixed length after speed change\n",
    "            augmented = AudioAugmenter.pad_or_truncate(augmented)\n",
    "            \n",
    "            return augmented\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in change_speed: {str(e)}\")\n",
    "            return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def augment(audio):\n",
    "        # Ensure input is fixed length\n",
    "        audio = AudioAugmenter.pad_or_truncate(audio)\n",
    "        \n",
    "        if not np.all(np.isfinite(audio)):\n",
    "            audio = np.nan_to_num(audio, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            audio = np.clip(audio, -1.0, 1.0)\n",
    "        \n",
    "        augmentation_list = ['noise', 'shift', 'speed']\n",
    "        num_augments = np.random.randint(1, 3)\n",
    "        selected_augments = np.random.choice(augmentation_list, num_augments, replace=False)\n",
    "        \n",
    "        augmented = audio.copy()\n",
    "        for aug_type in selected_augments:\n",
    "            try:\n",
    "                if aug_type == 'noise':\n",
    "                    augmented = AudioAugmenter.add_noise(augmented)\n",
    "                elif aug_type == 'shift':\n",
    "                    augmented = AudioAugmenter.time_shift(augmented)\n",
    "                elif aug_type == 'speed':\n",
    "                    augmented = AudioAugmenter.change_speed(augmented)\n",
    "                \n",
    "                if not np.all(np.isfinite(augmented)):\n",
    "                    augmented = np.nan_to_num(augmented, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                augmented = np.clip(augmented, -1.0, 1.0)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error during {aug_type} augmentation: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final length check\n",
    "        augmented = AudioAugmenter.pad_or_truncate(augmented)\n",
    "        return augmented.astype(np.float32)\n",
    "\n",
    "class ConsoleVisualizer:\n",
    "    \"\"\"Handles console-based visualization of plots\"\"\"\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm, labels):\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Header\n",
    "        print(f\"{'':>10}\", end='')\n",
    "        for label in labels:\n",
    "            print(f\"{label:>10}\", end='')\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Matrix\n",
    "        for i, label in enumerate(labels):\n",
    "            print(f\"{label:>10}\", end='')\n",
    "            for j in range(len(labels)):\n",
    "                if cm[i][j] == 0:\n",
    "                    color = Fore.WHITE\n",
    "                elif cm[i][j] == np.max(cm[i]):\n",
    "                    color = Fore.GREEN\n",
    "                else:\n",
    "                    color = Fore.YELLOW\n",
    "                print(f\"{color}{cm[i][j]:>10}{Style.RESET_ALL}\", end='')\n",
    "            print()\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_history(history):\n",
    "        print(\"\\nTraining History:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Extract relevant values from history\n",
    "        train_losses = [entry[\"loss\"] for entry in history if \"loss\" in entry]\n",
    "        eval_losses = [entry[\"eval_loss\"] for entry in history if \"eval_loss\" in entry]\n",
    "        eval_accuracies = [entry[\"eval_accuracy\"] for entry in history if \"eval_accuracy\" in entry]\n",
    "\n",
    "        # Determine the number of epochs\n",
    "        epochs = len(train_losses)  # Assuming loss is logged every epoch\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1:>2}: \"\n",
    "                  f\"Loss: {train_losses[epoch]:.4f} \"\n",
    "                f\"Val Loss: {eval_losses[epoch] if epoch < len(eval_losses) else 'N/A'} \"\n",
    "                f\"Acc: {eval_accuracies[epoch] if epoch < len(eval_accuracies) else 'N/A'}\")\n",
    "\n",
    "class AudioDataset(TorchDataset):\n",
    "    def __init__(self, audio_data, labels, feature_extractor, max_length=32000):\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def pad_or_truncate(self, audio):\n",
    "        \"\"\"Pad or truncate audio to fixed length\"\"\"\n",
    "        if len(audio) > self.max_length:\n",
    "            return audio[:self.max_length]\n",
    "        elif len(audio) < self.max_length:\n",
    "            return np.pad(audio, (0, self.max_length - len(audio)), 'constant')\n",
    "        return audio\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.audio_data[idx].astype(np.float32)\n",
    "        # Ensure fixed length\n",
    "        audio = self.pad_or_truncate(audio)\n",
    "        \n",
    "        inputs = self.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=16000,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=3):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        ).to(self.device)\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.label_map = {'crying': 0, 'screaming': 1, 'normal': 2}\n",
    "        self.config = self.model.config\n",
    "        self.augmenter = AudioAugmenter()\n",
    "        self.visualizer = ConsoleVisualizer()\n",
    "        self.max_length = 32000  # 2 seconds at 16kHz\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_or_truncate(audio, max_length=32000):\n",
    "        \"\"\"Pad or truncate audio to fixed length\"\"\"\n",
    "        if len(audio) > max_length:\n",
    "            return audio[:max_length]\n",
    "        elif len(audio) < max_length:\n",
    "            return np.pad(audio, (0, max_length - len(audio)), 'constant')\n",
    "        return audio\n",
    "\n",
    "    def load_audio_file(self, file_path, target_sr=16000):\n",
    "        \"\"\"Load and preprocess a single audio file\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(\n",
    "                file_path, \n",
    "                sr=target_sr, \n",
    "                mono=True,\n",
    "            )\n",
    "            # Normalize audio\n",
    "            audio = audio / (np.max(np.abs(audio)) + 1e-6)\n",
    "            # Ensure fixed length\n",
    "            audio = self.pad_or_truncate(audio, self.max_length)\n",
    "            return audio.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self, data_dir, metadata_file, augment=False):\n",
    "        \"\"\"Prepare dataset with fixed length audio\"\"\"\n",
    "        print(f\"Loading metadata from {metadata_file}\")\n",
    "        if not os.path.exists(metadata_file):\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
    "        \n",
    "        df = pd.read_csv(metadata_file)\n",
    "        print(f\"Loaded {len(df)} entries from metadata\")\n",
    "        \n",
    "        audio_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading audio files\"):\n",
    "            file_path = os.path.join(data_dir, row['file_name'])\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            audio = self.load_audio_file(file_path)\n",
    "            if audio is not None:\n",
    "                audio_data.append(audio)\n",
    "                labels.append(self.label_map[row['label']])\n",
    "                \n",
    "                if augment:\n",
    "                    augmented_audio = self.augmenter.augment(audio)\n",
    "                    # Ensure fixed length for augmented audio\n",
    "                    augmented_audio = self.pad_or_truncate(augmented_audio, self.max_length)\n",
    "                    audio_data.append(augmented_audio.astype(np.float32))\n",
    "                    labels.append(self.label_map[row['label']])\n",
    "        \n",
    "        dataset = AudioDataset(audio_data, labels, self.feature_extractor, self.max_length)\n",
    "        print(f\"Created dataset with {len(dataset)} examples\")\n",
    "        return dataset\n",
    "\n",
    "class AudioClassifier:\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=3):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        ).to(self.device)\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.label_map = {'crying': 0, 'screaming': 1, 'normal': 2}\n",
    "        self.config = self.model.config\n",
    "        self.augmenter = AudioAugmenter()\n",
    "        self.visualizer = ConsoleVisualizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_audio_file(file_path, target_sr=16000, max_duration=10):\n",
    "        \"\"\"Load and preprocess a single audio file with duration limit\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(\n",
    "                file_path, \n",
    "                sr=target_sr, \n",
    "                mono=True, \n",
    "                duration=max_duration\n",
    "            )\n",
    "            # Normalize audio\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "            return audio.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self, data_dir, metadata_file, augment=False):\n",
    "        \"\"\"Prepare dataset with custom AudioDataset class\"\"\"\n",
    "        print(f\"Loading metadata from {metadata_file}\")\n",
    "        if not os.path.exists(metadata_file):\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
    "        \n",
    "        df = pd.read_csv(metadata_file)\n",
    "        print(f\"Loaded {len(df)} entries from metadata\")\n",
    "        \n",
    "        audio_data = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading audio files\"):\n",
    "            file_path = os.path.join(data_dir, row['file_name'])\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Warning: File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            audio = self.load_audio_file(file_path)\n",
    "            if audio is not None:\n",
    "                audio_data.append(audio)\n",
    "                labels.append(self.label_map[row['label']])\n",
    "                \n",
    "                if augment:\n",
    "                    augmented_audio = self.augmenter.augment(audio)\n",
    "                    audio_data.append(augmented_audio.astype(np.float32))\n",
    "                    labels.append(self.label_map[row['label']])\n",
    "        \n",
    "        dataset = AudioDataset(audio_data, labels, self.feature_extractor)\n",
    "        print(f\"Created dataset with {len(dataset)} examples\")\n",
    "        return dataset\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save model and feature extractor\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "        # Save model properly\n",
    "        self.model.save_pretrained(path)\n",
    "    \n",
    "        # Save feature extractor\n",
    "        self.feature_extractor.save_pretrained(path)\n",
    "    \n",
    "        # Save label map\n",
    "        with open(os.path.join(path, \"label_map.json\"), \"w\") as f:\n",
    "            json.dump(self.label_map, f)\n",
    "        print(f\"Model saved at {path}\")\n",
    "\n",
    "    def train_kfold(self, dataset, output_dir, n_splits=5, use_wandb=False):\n",
    "        \"\"\"Train using k-fold cross validation\"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Get all indices\n",
    "        indices = np.arange(len(dataset))\n",
    "        \n",
    "        fold_metrics = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "            print(f\"\\nTraining Fold {fold + 1}/{n_splits}\")\n",
    "            \n",
    "            # Create fold datasets\n",
    "            train_fold = torch.utils.data.Subset(dataset, train_idx)\n",
    "            val_fold = torch.utils.data.Subset(dataset, val_idx)\n",
    "            \n",
    "            # Train on this fold\n",
    "            fold_output_dir = os.path.join(output_dir, f\"fold_{fold + 1}\")\n",
    "            trainer = self.train(train_fold, val_fold, fold_output_dir, use_wandb)\n",
    "            \n",
    "            # Evaluate fold\n",
    "            with torch.no_grad():  # Use no_grad() to save memory\n",
    "                metrics = trainer.evaluate()\n",
    "            fold_metrics.append(metrics)\n",
    "            \n",
    "            # Visualize fold results\n",
    "            self.visualizer.plot_training_history(trainer.state.log_history)\n",
    "\n",
    "            # Clear CUDA memory after training this fold\n",
    "            del trainer  # Delete trainer to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Print average metrics across folds\n",
    "        print(\"\\nAverage Metrics Across Folds:\")\n",
    "        avg_metrics = {\n",
    "            key: np.mean([fold[key] for fold in fold_metrics])\n",
    "            for key in fold_metrics[0].keys()\n",
    "        }\n",
    "        print(json.dumps(avg_metrics, indent=2))\n",
    "        \n",
    "        return avg_metrics\n",
    "    \n",
    "    def compute_metrics(self, pred):\n",
    "        \"\"\"Compute metrics for evaluation\"\"\"\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average='weighted'\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, output_dir, use_wandb=False):\n",
    "        \"\"\"Train with early stopping\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=25,  # Increased epochs for early stopping\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            gradient_accumulation_steps=4,\n",
    "            fp16 = True,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            logging_steps=50,\n",
    "            learning_rate=3e-5,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=500,\n",
    "            save_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            report_to=\"wandb\" if use_wandb else \"none\",\n",
    "        )\n",
    "\n",
    "        # Add early stopping\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        # Clear CUDA memory before training\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        trainer.train()\n",
    "        return trainer\n",
    "\n",
    "    def generate_performance_report(self, trainer, test_dataset, output_dir):\n",
    "        \"\"\"Generate and visualize performance metrics\"\"\"\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        preds = predictions.predictions.argmax(-1)\n",
    "        labels = predictions.label_ids\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = self.compute_metrics(predictions)\n",
    "        \n",
    "        # Generate and display confusion matrix\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        label_names = list(self.label_map.keys())\n",
    "        self.visualizer.plot_confusion_matrix(cm, label_names)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "def main():\n",
    "    output_dir = \"model_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        print(\"Initializing classifier...\")\n",
    "        classifier = AudioClassifier()\n",
    "        \n",
    "        print(\"\\nPreparing datasets with augmentation...\")\n",
    "        dataset = classifier.prepare_dataset(\n",
    "            \"Split_Data/train\",\n",
    "            \"Split_Data/train_metadata.csv\",\n",
    "            augment=True  # Enable augmentation\n",
    "        )\n",
    "        \n",
    "        # Train with k-fold cross validation\n",
    "        print(\"\\nStarting k-fold cross validation training...\")\n",
    "        metrics = classifier.train_kfold(dataset, output_dir, n_splits=5)\n",
    "        \n",
    "        # Save best model\n",
    "        best_model_path = os.path.join(output_dir, \"best_model\")\n",
    "        classifier.save_model(best_model_path)\n",
    "        print(f\"\\nBest model saved at: {best_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
